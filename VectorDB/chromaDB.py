import os
import re
import torch
import numpy as np
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings.base import Embeddings
from langchain.schema import Document
from transformers import AutoTokenizer, AutoModel
from typing import List

# âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
MODEL_NAME = "kakaobank/kf-deberta-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)

# âœ… ChromaDB ì €ì¥ ê²½ë¡œ ì„¤ì •
CHROMA_DB_DIR = "/data/ephemeral/VectorDB/kffDB"

# âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def clean_text(text):
    """ë¶ˆí•„ìš”í•œ ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# âœ… Mean Pooling ì ìš©í•˜ì—¬ ë¬¸ì¥ ì„ë² ë”© ìƒì„±
def mean_pooling(model_output, attention_mask):
    """Mean Poolingì„ ì ìš©í•˜ì—¬ ë¬¸ì¥ ì„ë² ë”© ìƒì„±"""
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return (token_embeddings * input_mask_expanded).sum(1) / input_mask_expanded.sum(1)

# âœ… DeBERTaë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
def generate_text_embeddings(texts):
    """DeBERTaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ì„ë² ë”©ì„ ìƒì„±"""
    if isinstance(texts, str):
        texts = [texts]

    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=512)
    
    with torch.no_grad():
        model_output = model(**inputs)
    
    embeddings = mean_pooling(model_output, inputs['attention_mask'])
    return embeddings.cpu().numpy().tolist()

# âœ… Chromaì—ì„œ ì‚¬ìš©í•  ì»¤ìŠ¤í…€ ì„ë² ë”© í•¨ìˆ˜ ì •ì˜
class DeBERTaEmbeddingFunction(Embeddings):
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return generate_text_embeddings(texts)

    def embed_query(self, text: str) -> List[float]:
        return generate_text_embeddings(text)

# âœ… PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ChromaDBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def process_pdf_to_chromadb(pdf_path, collection_name):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì„ë² ë”© ìƒì„± í›„ ChromaDBì— ì €ì¥."""
    print(f"\nğŸ”„ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")

    loader = PyPDFLoader(pdf_path)
    pdf_docs = loader.load()
    print(f"âœ… PDFì—ì„œ {len(pdf_docs)}ê°œì˜ í˜ì´ì§€ë¥¼ ë¡œë“œí•¨.")

    text_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=100)
    split_docs = []
    
    for doc in pdf_docs:
        cleaned_text = clean_text(doc.page_content)
        chunks = text_splitter.split_text(cleaned_text)
        for chunk in chunks:
            split_docs.append(Document(page_content=chunk, metadata=doc.metadata))

    print(f"âœ… ì´ {len(split_docs)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±ë¨.")

    vectorstore = Chroma(
        collection_name=collection_name,
        embedding_function=DeBERTaEmbeddingFunction(),
        persist_directory=CHROMA_DB_DIR,
    )

    texts = [doc.page_content for doc in split_docs]
    metadatas = [doc.metadata for doc in split_docs]

    if not texts:
        print("âŒ ì €ì¥í•  í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    print("âœ… ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = generate_text_embeddings(texts)
    
    if not embeddings or np.all(np.array(embeddings) == 0):
        print("âŒ ì„ë² ë”©ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ ë˜ëŠ” ì…ë ¥ì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ (ì´ {len(embeddings)}ê°œ)")
    vectorstore.add_texts(texts=texts, metadatas=metadatas)
    vectorstore.persist()

    num_docs = vectorstore._collection.count()
    print(f"âœ… í˜„ì¬ ChromaDBì— ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜: {num_docs}")

    saved_data = vectorstore._collection.get(limit=3)
    print("âœ… ì €ì¥ëœ ë°ì´í„° ìƒ˜í”Œ (ìµœëŒ€ 3ê°œ):", saved_data.get("documents", []))

    print(f"âœ… PDF {os.path.basename(pdf_path)} ì²˜ë¦¬ ì™„ë£Œ ë° ì €ì¥ë¨.")

# âœ… ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬
def process_all_pdfs_in_directory(directory, collection_name):
    """ì§€ì •ëœ ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF íŒŒì¼ì„ ì²˜ë¦¬."""
    print(f"\nğŸ“‚ ë””ë ‰í† ë¦¬ ë‚´ PDF ì²˜ë¦¬ ì‹œì‘: {directory}")

    pdf_files = [os.path.join(root, file)
                 for root, _, files in os.walk(directory)
                 for file in files if file.endswith(".pdf")]

    if not pdf_files:
        print("âŒ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"âœ… ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ ë°œê²¬ë¨.")

    for pdf_path in pdf_files:
        process_pdf_to_chromadb(pdf_path, collection_name)
